import matplotlib
matplotlib.use('PS')

from cdt.metrics import precision_recall
import numpy as np

from preparation import data_preperation
from networks import p_x_z, p_y_z, q_y_x, q_z_yx
from evaluation import Evaluator, get_y0_y1
from initialisation import init_qz
from preparation import data_preperation

from argparse import ArgumentParser
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split

import numpy as np
from numpy import savetxt
import matplotlib.pyplot as plt
from collections import defaultdict
from sklearn.metrics import log_loss

import torch
from torch.distributions import normal
from torch import optim
import csv

import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import bernoulli, normal
import sys

from scipy.stats import ttest_rel
from scipy.stats import kruskal
from scipy.stats import ttest_ind

import time

from cdt.metrics import SHD
from numpy.random import randint

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# THE CODE

def network(data, data_function, y):
    
    '''
    This is the training network function that can be called whenever necessary. Most imporantly, it will output
    a trained functional generative network that can be used to do interventions. Also, it will return the trainings
    data used in for the training procedure.
    
    Input: data (data generated by the functional causal models), the data preperation function and y, the target
    variable.
    
    Output: the functional network p(y|x) and the trainings data.
    
    N.B.: it is advised to increase h_dim with the number of variables. For 3 variables 32 hidden dims seems to be sufficient, for 
    5/6 variables (64 hidden dims); 10/11 variables (128 hidden dims); 20/21 variables (256 hidden dims). 
    '''
    
    # Set hyperparameters
    z_dim = 10
    h_dim = 256
    lr = 0.00001
    decay = 0.001
    batch = 100
    epochs = 500
    
    x_train, y_train, t_train, xtr, ttr, ytr = data_preperation(data, target = y)
    binfeats = []
    contfeats = [i for i in range(len(x_train.T))]

    # Initialise the networks
    x_dim = len(binfeats) + len(contfeats)

    p_x_z_dist = p_x_z(dim_in=z_dim, nh=3, dim_h=h_dim, dim_out_bin=len(binfeats),
                       dim_out_con=len(contfeats))

    #p_t_z_dist = p_t_z(dim_in=z_dim, nh=1, dim_h=h_dim, dim_out=1)
    p_y_z_dist = p_y_z(dim_in=z_dim, nh=3, dim_h=h_dim, dim_out=1)
    #q_t_x_dist = q_t_x(dim_in=x_dim, nh=1, dim_h=h_dim, dim_out=1)
    q_y_x_dist = q_y_x(dim_in=x_dim, nh=3, dim_h=h_dim, dim_out=1)

    q_z_yx_dist = q_z_yx(dim_in=len(binfeats) + len(contfeats) + 1, nh=3, dim_h=h_dim,
                           dim_out=z_dim)

    p_z_dist = normal.Normal(torch.zeros(z_dim), torch.ones(z_dim))

    # init q_z inference
    qz = init_qz(q_z_yx_dist, p_z_dist, ytr, ttr, xtr)
    
    #Set n_epochs and batch size
    n_epoch = epochs
    n_iter_per_epoch = 10
    M = 100 #batch
    loss = defaultdict(list)
    idx = list(range(xtr.shape[0]))

    # Create optimizer
    params = list(p_x_z_dist.parameters()) + \
             list(p_y_z_dist.parameters()) + \
             list(q_y_x_dist.parameters()) + \
             list(q_z_yx_dist.parameters())

    # Adam is used, like original implementation, in paper Adamax is suggested
    optimizer = optim.Adam(params, lr=lr, weight_decay=decay)
    
    for epoch in range(n_epoch):
        loss_sum = 0.
        np.random.shuffle(idx)
        for j in range(n_iter_per_epoch):
            # select random batch
            batch = np.random.choice(idx, 100) #change 100 to M 
            x_batch = x_train[batch]
            y_batch = y_train[batch]
            t_batch = t_train[batch]

            # inferred distribution over z
            xy = torch.cat((x_batch, y_batch), 1)
            z_infer = q_z_yx_dist(xy=xy.float())  #xy = torch.cat((x_train, y_train), 1) as input
            z_infer_sample = z_infer.sample()

            # RECONSTRUCTION LOSS
            # p(x|z)
            x_bin, x_con = p_x_z_dist(z_infer_sample)
            l1 = x_bin.log_prob(x_batch[:, :len(binfeats)]).sum(1)
            loss['Reconstr_x_bin'].append(l1.sum().cpu().detach().float())
            l2 = x_con.log_prob(x_batch[:, -len(contfeats):]).sum(1)
            loss['Reconstr_x_con'].append(l2.sum().cpu().detach().float())

            # p(t|z)
            #t = p_t_z_dist(z_infer_sample)
            #l3 = t.log_prob(t_batch).squeeze()
            #loss['Reconstr_t'].append(l3.sum().cpu().detach().float())

            # p(y|t,z)
            # for training use t_batch, in out-of-sample prediction this becomes t_infer
            y = p_y_z_dist(z_infer_sample)
            
            y_infer_sample = y.sample()
            
            l4 = y.log_prob(y_batch).squeeze()
            loss['Reconstr_y'].append(l4.sum().cpu().detach().float())

            # REGULARIZATION LOSS
            # p(z) - q(z|x,t,y)
            # approximate KL
            l5 = (p_z_dist.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)).sum(1)
            # Analytic KL (seems to make overall performance less stable)
            # l5 = (-torch.log(z_infer.stddev) + 1/2*(z_infer.variance + z_infer.mean**2 - 1)).sum(1)
            loss['Regularization'].append(l5.sum().cpu().detach().float())

            # q(y|x,t)
            y_infer, mu = q_y_x_dist(x_batch.float())
            
            #Approximate the KL of p(y|z) - q(y|x)  =====   p(y|x) - q(y|z)
            # p(y|x) - q(y|z)
            #l8 = y_infer.log_prob(y_infer_sample) - y.log_prob(y_infer_sample).sum(1)
            
            
            # AUXILIARY LOSS
            # q(t|x)
            #t_infer = q_t_x_dist(x_batch.float())
            #l6 = t_infer.log_prob(t_batch).squeeze()
            #loss['Auxiliary_t'].append(l6.sum().cpu().detach().float())

            #INCLUDE L8
            
            l7 = y_infer.log_prob(y_batch).squeeze()
            loss['Auxiliary_y'].append(l7.sum().cpu().detach().float())
            
            #l8 = (p_z_dist.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)).sum(1)
            
            l8 = y_infer.log_prob(y_infer_sample) - y.log_prob(y_infer_sample).sum(1)
            loss['Auxiliary_y_KL'].append(l8.sum().cpu().detach().float())
            #loss['Auxiliary_y'].append(l7.sum().cpu().detach().float())

            # Total objective
            # inner sum to calculate loss per item, torch.mean over batch
            loss_mean = torch.mean(l1 + l2 + l4 + l5 + l7 + l8)
            loss['Total'].append(loss_mean.cpu().detach().numpy())
            objective = -loss_mean

            optimizer.zero_grad()
            # Calculate gradients
            objective.backward()
            # Update step
            optimizer.step()
        
    return q_y_x_dist, x_train #, p_z_dist, z_infer, y_infer, y
    

def intervention_on_y(data, model, k, base_case):
    
    '''
    This function performs the intervention on the data. First, it determines if the base
    case holds. By that I mean the 'actual' predicted normalised value for y. This corresponds 
    to regimen F_naught. This is essentially the a normalised version of the generated data --
    the data as it would have been without any intervention.
    
    If the condition does not hold, we move to the next block. On every iteration, an intervention
    is created by setting the corresponding variable value to 0 using np.zeros(). Thus, for the first
    iteration, we set x_1 to 0, on the second iteration x_2, then x_3 to 0 etc. Every time a distribution
    is generated by our model from which we sample the predicted y values. Ideally, if a variable has no
    effect on y, we expect to see little difference between the base_case value and the predicted value. 
    For instance, if x_1 does not affect x_3 and the base_value for x_3 is 0.5, we would expect the predicted
    value for x_3 (i.e. y) to be close to 0.5 as well. However, if x_1 does effect x_3, we expect to see 
    the predicted value for x_3 (i.e. y) to go up or down w.r.t. the base_value of 0.5. Typically, a difference
    in value between y_no_intervent and y_intervent is somewhere in the range of +-0.05 w.r.t. the base_value. 
    
    Input: data, the CEVAE pre-trained model and whether or not we want the base case (boolean)
    
    Output: Predicted target value per intervened variable.
    '''
    
    if base_case == True:
        #normalise
        data = data.detach().numpy()
        for i in range(len(data.T)):
            data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
        data = torch.tensor(data)
        y, mu = model(data.float())
        return mu #.mean()
    
    else:
        #intervene here
        data = data.detach().numpy()
        for i in range(len(data.T)):
            data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
        data = torch.tensor(data)
        data[:,k] = torch.zeros(len(data))
        y, mu = model(data.float())

        return mu #.mean()

# get the means of the various outputs to counterbalance too much variation



# EFFECT ESTIMATION

def var_reduction(function, data, model, run_time, target):
    '''
    This function computes the estimated mean value for y (the var of interest).
    By this I mean the variability of the predicted target value. If, for instance, the effect of x_1 on x_3
    is very strong, the effect is apparent and clear when doing the intervention. However, when the effect is
    not so strong, it is necessary to be sure that the value we obtain actually represents the true effect of x_1
    on x_3. If the effect is strong, the predicted target value (i.e. x_3) with intervention will be more than or less
    than 0.05. However, if the effect is smaller, for, say x_2 on x_5, the predicted effect can sometimes by 0.04, and 
    sometimes 0.06. In order to be sure that the effect is actually there or simply the result of some random noise, we
    want to compare the averaged values rather than values of predicted instances which can fluctuate. 
    
    It also creates a dictionary containing the names corresponding to the variables
    
    Input: the function intervention_on_y, data (train data) and p(y|x) model, the run_time (how often we want to loop),
    the dim (the number of vars) and y_var (the name of the target variable y (e.g. x_1, x_4 etc.))
    
    Output: list of mean of means per variable intervention
    '''
    y_var = target
    
    idx = list(range(data.shape[0]))
    np.random.shuffle(idx)
    
    mean_list = []
    temp_mean_lst = []
    name_dict = {}
    update_value = 0
    aux_dict = {}
    means_total_list = []
    
    for i in range(run_time):
        batch = np.random.choice(idx, len(data)) #change 100 to M 
        data = data[batch]
        element = function(data = data, model = model, k = 0, base_case = True).detach().numpy().mean()
        temp_mean_lst.append(element)
        
        
    
    #return temp_mean_lst

    means_total_list.append(temp_mean_lst)
    mean_list.append(np.mean(temp_mean_lst))
    
    
    value = np.mean(temp_mean_lst)
    
    for j in range(len(data.T)):
        temp_mean_lst = []
        batch = np.random.choice(idx, len(data)) #change 100 to M 
        data = data[batch]
        for i in range(run_time):
            element = function(data = data, model = model, k = j, base_case = False).detach().numpy().mean()
            temp_mean_lst.append(element)
        mean_list.append(np.mean(temp_mean_lst))
        means_total_list.append(temp_mean_lst)
        
    for element in mean_list:
        if update_value == 0:
            name_dict['x_' + str(y_var) + '*'] = element
            aux_dict[update_value] = element
        elif update_value == y_var:
            update_value += 1
            name_dict['x_' + str(update_value)] = element
            aux_dict[update_value] = element
        else:
            name_dict['x_' + str(update_value)] = element
            aux_dict[update_value] = element
        update_value += 1
    
    aux_dict[y_var] = list(aux_dict.values())[0]
    del aux_dict[0]
    

    aux_dict = {k: v for k, v in sorted(aux_dict.items(), key=lambda item: item[0])}
    
    # also return a sorted dict
    sorted_dict = {k: v for k, v in sorted(name_dict.items(), key=lambda item: item[1])}

    return mean_list, sorted_dict, name_dict, aux_dict, means_total_list



def adjacency_matrix(aux_dict, target):
    '''
    This function converts the output of the model to an adjacency matrix from which we can retrieve the
    causal model. It takes the list of output values, the variable in question and the reordering as its
    input and returns a matrix for that variable.
    
    Input: mean output_list of estimated values, the name of the variable y_var (i.e. x_6 = y_var = 5),
    the ordered list. 
    
    Output: a 2D adjacency matrix (one containing the abs. values w.r.t. to the target; another one
    containing the relative values).
    '''

    val_of_target = aux_dict[target]
    val_of_target
    abs_adj_matrix = np.array(np.zeros((len(aux_dict),len(aux_dict))))

    # absolute valued matrix
    abs_adj_matrix[:,target-1] = abs(np.array(list(aux_dict.values())) - val_of_target)
    abs_adj_matrix[target-1,:] = abs(np.array(list(aux_dict.values())) - val_of_target)
    
    # regular valued matrix
    reg_adj_matrix = np.array(np.zeros((len(aux_dict),len(aux_dict))))
    reg_adj_matrix[:,target-1] = np.array(list(aux_dict.values()))
    reg_adj_matrix[target-1,:] = np.array(list(aux_dict.values())) 
    
    # convert the matrices to pd.df 
    matrix_reg = pd.DataFrame(reg_adj_matrix)
    matrix_reg.columns = ['x' + str(i + 1) for i in range(len(matrix_reg))]
    matrix_reg.index = list(matrix_reg.columns)
    matrix_reg
    
    matrix_abs = pd.DataFrame(abs_adj_matrix)
    matrix_abs.columns = ['x' + str(i + 1) for i in range(len(matrix_abs))]
    matrix_abs.index = list(matrix_abs.columns)
    matrix_abs
    
    return abs_adj_matrix, reg_adj_matrix, matrix_reg, matrix_abs

#abs_adj_matrix, reg_adj_matrix, matrix_reg, matrix_abs = adjacency_matrix(aux_dict, target = 2)

#matrix_abs


def determine_threshold(matrix, target):
    '''
    This function determines whether or not some value inn the absolute valued matrix is greater than the mean value of
    all the variables. If this is the case, it will output these variables. 
    '''
    return np.where(matrix.iloc[target-1,:] > matrix.iloc[target-1,:].mean()), np.where(matrix.iloc[target-1,:] > matrix.iloc[target-1,:].median())



def normalise_data(data):
    '''
    This function takes the unormalised data as input and outputs normalised data.
    '''
    data = np.array(data)
    for i in range(1,len(data.T)):
        data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
    data = pd.DataFrame(data)
    return data

## PREDICTOR (MAIN)

def hypothesis_testing(data, model, target):    
    parents = []
    remain_list = []
    cond_list = []
    
    # Obtain the default distribution F_naught
    for i in range(len(data.T)):
        data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
    data = torch.tensor(data)
    distribution_normal, mu1 = model(data.float())
    y_normal = distribution_normal.sample().detach().numpy()

    var_counter = 1

    for j in range(len(data.T)):
        same, different = 0, 0
        data2 =  x_train.detach().clone()
        for i in range(len(data2.T)):
            data2[:,i] = (data2[:,i]-min(data2[:,i]))/(max(data2[:,i])-min(data2[:,i]))

        # Obtain the interventional distribution 
        for i in range(100):
            data2[:,j] = torch.zeros(len(data2))
            data2 = torch.tensor(data2)
            distribution_interven, mu2 = model(data2.float())

            y_interven = distribution_interven.sample().detach().numpy()
            stat, p = ttest_rel(y_normal,y_interven)
            #print('stat=%.3f, p=%.3f' % (stat, p))
            if p > 0.05:
                #print('Probably the same distribution')
                same += 1
            else:
                #print('Probably different distributions')
                different += 1

            if var_counter == target:
                var_counter += 1

        print('x', var_counter, '--------------------------')    
        print('For x', var_counter, 'there are', same, 'same. There are', different, 'different')
        var_counter += 1
        
        if different > same:
            parents.append(var_counter-1)
            cond_list.append(j)
        
    #remain_list = [i for i in range(len(data.T)) if i != cond_list and i != cond_list]
    remain_list = [i for i in range(len(data.T)) if i not in cond_list]
    return parents, remain_list, cond_list
    
    

def hypothesis_testing_run(data, model, remain_list, conditional_list):   
    """
    This function is very similar to the previous hypothesis testing function. However, the main difference is that this
    function works with a conditional distribution rather than with the original distribution as was the case in the previous
    function. Thus, in the previous function, we compared P(y | F_naught) with P(y | F_interven). However, there we might only
    detect one or two substantial differences between the densities. In order to be certain that we did not miss anything, we 
    condition on the causal effect variable and subsequently intervene upon the remaining variables. Thus, suppose that we found
    that x_{5} has an effect on x_{2} on the first round. In that case, we compared P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0)
    with P(x_{2} | x_{1}, x_{3}, x_{4}, x_{5}). 
    And since P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0) != P(x_{2} | x_{1}, x_{3}, x_{4}, x_{5}), we concluded that x_{5} has an 
    effect on the target, x_{2}. 
    
    Now, suppose that x_{1} as well as x_{4} also have an effect on x_{5}, but this effect got 'canceled' out by x_{5}, because
    the effect of x_{5} on x_{2} significantly larger than either the effects of x_{1} or x_{4} on x_{2}, thereby reducing their
    quantitative impact. In that case, we can generate a new distribution P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0) and compare
    this distribution with P(x_{2} | do(x_{1})=0, x_{3}, x_{4}, do(x_{5} = 0), P(x_{2} | x_{1}, do(x_{3}=0), x_{4}, do(x_{5} = 0)
    and P(x_{2} | x_{1}, x_{3}, do(x_{4}=0), do(x_{5} = 0), respectivly. Since relativly large effect of x_{5} has now been eliminated,
    we simply therefore simply condition on the derived distribution. Then, since x_{1} has an effect on x_{2}, we will find that
    
            P(x_{2} | do(x_{1}=0), x_{3}, x_{4}, do(x_{5} = 0) != P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0). 
    
    In similar vein, for x_{4} we will find that 
    
            P(x_{2} | x_{1}, x_{3}, do(x_{4}=0), do(x_{5} = 0) != P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0). 
        
    Since x_{3} has not effect on x_{2}, we will also find that:
    
            P(x_{2} | x_{1}, do(x_{3}=0), x_{4}, do(x_{5} = 0) = P(x_{2} | x_{1}, x_{3}, x_{4}, do(x_{5} = 0)
            
    The function takes as its input:
    
    Arg1: the data
    Arg2: the model, p(y|x)_dist
    Arg3: the name of the target variable
    Arg4: remain list. This is a list of variables whose effect could not be established in the first run. 
    (e.g. in our example above, this list contains [0,1,2], variables x_{1}, x_{3} and x_{4}. 
    Arg5: conditional_list. This is a list of variable(s) whose effect could be determined in the first run.
    (e.g. in our example from above, this is [3], corresponding variable x_{5})
    
    Return: Set of parents
    
    """
    parents = []
    for i in range(len(data.T)):
        data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
    
    for element in conditional_list:
        data[:,element] = torch.zeros(len(data))

    data = torch.tensor(data)
    distribution_normal, mu1 = model(data.float())
    y_normal = distribution_normal.sample().detach().numpy()
    var_counter = 1

    for j in remain_list:
        same = 0
        different = 0 
        data2 = data
        for i in range(len(data2.T)):
            data2[:,i] = (data2[:,i]-min(data2[:,i]))/(max(data2[:,i])-min(data2[:,i]))

        for element in conditional_list:
            data2[:,element] = torch.zeros(len(data2))

        data2[:,j] = torch.zeros(len(data))
        for i in range(100):
            data2 = torch.tensor(data2)
            distribution_interven, mu2 = model(data2.float())
            y_interven = distribution_interven.sample().detach().numpy()

            stat, p = ttest_rel(y_normal,y_interven)
            #print('stat=%.3f, p=%.3f' % (stat, p))
            if p > 0.05:
                #print('Probably the same distribution')
                same += 1
            else:
                #print('Probably different distributions')
                different += 1

            if var_counter == target:
                var_counter += 1

        print('x', var_counter, '--------------------------')    
        print('For x', var_counter, 'there are', same, 'same. There are', different, 'different')
        var_counter += 1
        
        print(j)
        
        if different > same:
            #parents.append(var_counter-1)
            parents.append(j)
        
    parents = [i for i in range(len(data.T)) if i not in parents] # and i != cond_list]

    return parents #, remain_list
    


def hyp_test_main(data, model, target):

    total_of_runs = []
    for i in range(10):
        parents, remain_list, cond_list = hypothesis_testing(data = data, model = model, target = target)
        total_of_runs.append(cond_list)

    total_of_runs

    total_of_runs_exp = [j for i in total_of_runs for j in i]

    total_of_runs_exp

    final_output_list = []
    for element in range(len(data.T)):
        score = total_of_runs_exp.count(element)
        if score > 8:
            final_output_list.append(element)

    remaining = [i for i in range(len(data.T)) if i not in final_output_list]
    
    if len(final_output_list) > 0:
        total_of_runs_run = []
        for i in range(10):
            total_of_runs_run.append(hypothesis_testing_run(data = data, model = model, remain_list = remaining, conditional_list = final_output_list))

        total_of_runs_exp_run = [j for i in total_of_runs for j in i]

        final_output_list = []
        for element in range(len(data.T)):
            score = total_of_runs_exp_run.count(element)
            if score > 8:
                final_output_list.append(element)

        remaining = [i for i in range(len(data.T)) if i not in final_output_list]


    return final_output_list, remaining


def main_for_var(data, target, data_preperation, network):
    """
    This function is the main function for one variable. 
    
    Arg1: Data
    Arg2: The target of interest (not zero indexed)
    Arg3: Data preperation function
    Arg4: Network function (Function to train)
    
    Return1: trainings_data
    Return2: Trained network 
    """
    
    x_train, y_train, t_train, xtr, ttr, ytr = data_preperation(data, target)
    q_y_x_dist, x_train = network(data, data_preperation, target)
    return x_train, q_y_x_dist


def obtain_distribution(data, model, intervention_list = []):
    """
    This function returns a distribution. The function can be used to obtain the interventional and non-interventional
    distributions.
    
    Arg1: Data
    Arg2: The model we can use to generate data from
    Arg3: A list of the variables we want to intervene on
    
    Return: Sample of the target distribution
    """
    
    # Obtain the default distribution F_naught
    for i in range(len(data.T)):
        data[:,i] = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))
    data = torch.tensor(data)

    for i in intervention_list:
        data[:,i] = torch.zeros(len(data))
    distribution_normal, mu1 = model(data.float())
    return distribution_normal.sample().detach().numpy(), data


class MMDloss(torch.nn.Module):
    """**[torch.nn.Module]** Maximum Mean Discrepancy Metric to compare
    empirical distributions.
    The MMD score is defined by:
    .. math::
        \\widehat{MMD_k}(\\mathcal{D}, \\widehat{\\mathcal{D}}) = 
        \\frac{1}{n^2} \\sum_{i, j = 1}^{n} k(x_i, x_j) + \\frac{1}{n^2}
        \\sum_{i, j = 1}^{n} k(\\hat{x}_i, \\hat{x}_j) - \\frac{2}{n^2} 
        \\sum_{i,j = 1}^n k(x_i, \\hat{x}_j)
    where :math:`\\mathcal{D} \\text{ and } \\widehat{\\mathcal{D}}` represent 
    respectively the observed and empirical distributions, :math:`k` represents
    the RBF kernel and :math:`n` the batch size.
    Args:
        input_size (int): Fixed batch size.
        bandwiths (list): List of bandwiths to take account of. Defaults at
            [0.01, 0.1, 1, 10, 100]
        device (str): PyTorch device on which the computation will be made.
            Defaults at ``cdt.SETTINGS.default_device``.
    Inputs: empirical, observed
        Forward pass: Takes both the true samples and the generated sample in any order 
        and returns the MMD score between the two empirical distributions.
        + **empirical** distribution of shape `(batch_size, features)`: torch.Tensor
          containing the empirical distribution
        + **observed** distribution of shape `(batch_size, features)`: torch.Tensor
          containing the observed distribution.
    Outputs: score
        + **score** of shape `(1)`: Torch.Tensor containing the loss value.
    .. note::
        Ref: Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, 
        B., & Smola, A. (2012). A kernel two-sample test.
        Journal of Machine Learning Research, 13(Mar), 723-773.
    Example:
        >>> from cdt.utils.loss import MMDloss
        >>> import torch as th
        >>> x, y = th.randn(100,10), th.randn(100, 10)
        >>> mmd = MMDloss(100)  # 100 is the batch size
        >>> mmd(x, y)
        0.0766
    """

    def __init__(self, input_size, bandwidths=None):
        """Init the model."""
        super(MMDloss, self).__init__()
        if bandwidths is None:
            bandwidths = torch.Tensor([0.01, 0.1, 1, 10, 100])
        else:
            bandwidths = bandwidths
        s = torch.cat([torch.ones([input_size, 1]) / input_size,
                    torch.ones([input_size, 1]) / -input_size], 0)

        self.register_buffer('bandwidths', bandwidths.unsqueeze(0).unsqueeze(0))
        self.register_buffer('S', (s @ s.t()))

    def forward(self, x, y):
        X = torch.cat([x, y], 0)
        # dot product between all combinations of rows in 'X'
        XX = X @ X.t()
        # dot product of rows with themselves
        # Old code : X2 = (X * X).sum(dim=1)
        # X2 = XX.diag().unsqueeze(0)
        X2 = (X * X).sum(dim=1).unsqueeze(0)
        # print(X2.shape)
        # exponent entries of the RBF kernel (without the sigma) for each
        # combination of the rows in 'X'
        exponent = -2*XX + X2.expand_as(XX) + X2.t().expand_as(XX)
        b = exponent.unsqueeze(2).expand(-1,-1, self.bandwidths.shape[2]) * -self.bandwidths
        lossMMD = torch.sum(self.S.unsqueeze(2) * b.exp())
        return lossMMD

def direction(P,Q):
    """
    This function computes the mmd between the marginal distribution of a variable and the post-interventional 
    distribution of that variable given some other variable. The idea being that, by the principle of independence
    of cause and mechanism, if the difference between P and Q is small, than the intervention will not have had
    a big impact on the variable. 
    
    Input: P = p(x)
    Input: Q = p(x|do(y=0))
    
    Return: maximum mean discrepancy score
    """

    mmd = MMDloss(len(P))  # 100 is the batch size
    return mmd(P.float(), Q.float())

# Example
x, y = torch.randn(100,10), torch.randn(100, 10)
mmd = MMDloss(100)  # 100 is the batch size
mmd(x, y)



def edge_detection(child, child_train, parent, block = [], mmd_threshold = 0.1):
    """
    This function determines whether there is an edge between two variables X and Y. 
    
    Arg1: Child distribution
    Arg2: Child train data
    Arg3: Parent to be intervened upon, must be a list
    Arg4: Additional: interventional for the additional
    Arg5: Threshold value
    
    Return: 0 if there is no edge, 1 otherwise
    """
    
    start_time = time.time()
    couter_no = 0
    mmd_val_avg = []
    
    for i in range(5):
        y_normal, data_normal = obtain_distribution(data = child_train.clone(), model = child, intervention_list = block)
        y_interven, data_interven = obtain_distribution(data = child_train.clone(), model = child, intervention_list = parent)
        mmd_value = direction(torch.tensor(y_normal[:4000]), torch.tensor(y_interven[:4000]))
        print('The difference between of the Post-I and Pre-I is distribution is:', mmd_value)
        mmd_val_avg.append(mmd_value)
        
        if mmd_value < float(mmd_threshold):
            #print('No parent')
            couter_no += 1
    print("--- Execution time : %4.4s seconds ---" % (time.time() - start_time))    
        
    if couter_no > 3:  #change to 3/4
        return 0, y_interven, np.mean(mmd_val_avg)
    else:
        return 1, y_interven, np.mean(mmd_val_avg)


